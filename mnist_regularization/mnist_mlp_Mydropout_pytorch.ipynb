{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import pickle\n",
    "\n",
    "class IO:\n",
    "    def __init__(self, file_name):\n",
    "        self.file_name = file_name\n",
    "        \n",
    "    def to_pickle(self, obj):\n",
    "        with open(self.file_name, 'wb') as output:\n",
    "            pickle.dump(obj, output, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    def read_pickle(self):\n",
    "        with open(self.file_name, 'rb') as input_:\n",
    "            obj = pickle.load(input_)\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), \\\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = datasets.MNIST(root='data/', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='data/', train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyDropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(MyDropout, self).__init__()\n",
    "        self.p = p\n",
    "    def forward(self, input):\n",
    "        if not self.training:\n",
    "            return input\n",
    "        selected_ = torch.Tensor(input.shape).uniform_(0,1)>self.p\n",
    "        selected_num = torch.sum(selected_)\n",
    "        if selected_num > 0:\n",
    "            multiplier_ = torch.numel(selected_)/selected_num\n",
    "        else:\n",
    "            multiplier_ = 0\n",
    "        if input.is_cuda:\n",
    "            selected_ = Variable(selected_.type(torch.cuda.FloatTensor), requires_grad=False)\n",
    "        else:\n",
    "            selected_ = Variable(selected_.type(torch.FloatTensor), requires_grad=False)\n",
    "        return torch.mul(selected_,input) * multiplier_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getData(testloader):\n",
    "    return iter(testloader).next()\n",
    "\n",
    "def accuracy_score(y_true, y_pred, verbose=False):\n",
    "    if not verbose:\n",
    "        return np.mean(y_true == y_pred)\n",
    "    else:\n",
    "        return np.array([np.mean(y_pred_test[y_test == i] == i) for i in range(10)])\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_layers=[800, 800], droprates=[0, 0]):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add_module(\"dropout0\",MyDropout(p=droprates[0]))\n",
    "        self.model.add_module(\"input\", nn.Linear(28*28, hidden_layers[0]))\n",
    "        self.model.add_module(\"tanh\", nn.Tanh())\n",
    "        for i,d in enumerate(hidden_layers[:-1]):\n",
    "            self.model.add_module(\"dropout_hidden\"+str(i+1), MyDropout(p=droprates[1]))\n",
    "            self.model.add_module(\"hidden\"+str(i+1), nn.Linear(hidden_layers[i], hidden_layers[i+1]))\n",
    "            self.model.add_module(\"tanh_hidden\"+str(i+1), nn.Tanh())\n",
    "        self.model.add_module(\"final\",nn.Linear(hidden_layers[-1], 10))\n",
    "        #self.model.add_module(\"logsoftmax\", nn.LogSoftmax(dim=1))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], 28*28)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "class MLPClassifier:\n",
    "    def __init__(self, hidden_layers=[800, 800], droprates=[0, 0], batch_size=100, max_epoch=10, \\\n",
    "                 lr=0.1, momentum=0):\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.droprates = droprates\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epoch = max_epoch\n",
    "        self.model = MLP(hidden_layers=hidden_layers, droprates=droprates)\n",
    "        self.model.cuda()\n",
    "        #self.criterion = nn.NLLLoss()\n",
    "        self.criterion = nn.CrossEntropyLoss().cuda()\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=lr, momentum=momentum)\n",
    "        self.loss_ = []\n",
    "        self.test_accuracy = []\n",
    "        self.test_error = []\n",
    "        \n",
    "    def fit(self, trainset, testset, verbose=True):\n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=self.batch_size, shuffle=True)\n",
    "        testloader = torch.utils.data.DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
    "        X_test, y_test = getData(testloader)\n",
    "        X_test = X_test.cuda()\n",
    "        print(self)\n",
    "        for epoch in range(self.max_epoch):\n",
    "            running_loss = 0\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.data[0]\n",
    "            self.loss_.append(running_loss / len(trainloader))\n",
    "            if verbose:\n",
    "                print('Epoch {} loss: {}'.format(epoch+1, self.loss_[-1]))\n",
    "            y_test_pred = self.predict(X_test).cpu()\n",
    "            self.test_accuracy.append(np.mean(y_test == y_test_pred))\n",
    "            self.test_error.append(int(len(testset)*(1-self.test_accuracy[-1])))\n",
    "            if verbose or epoch + 1 == self.max_epoch:\n",
    "                print('Test error: {}; test accuracy: {}'.format(self.test_error[-1], self.test_accuracy[-1]))\n",
    "        print('Finished Training.')\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        model = self.model.eval()\n",
    "        outputs = model(Variable(x))\n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "        model = self.model.train()\n",
    "        return pred\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Hidden layers: {}; dropout rates: {}'.format(self.hidden_layers, self.droprates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layers = [800, 800]\n",
    "mlp1 = [MLPClassifier(hidden_layers, droprates=[0, 0], max_epoch=1500), \n",
    "        MLPClassifier(hidden_layers, droprates=[0, 0.5], max_epoch=1500),\n",
    "        MLPClassifier(hidden_layers, droprates=[0.2, 0.5], max_epoch=1500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(mlp1[0].model)\n",
    "mlp1[0].fit(trainset, testset);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (model): Sequential(\n",
      "    (dropout0): MyDropout(\n",
      "    )\n",
      "    (input): Linear(in_features=784, out_features=800)\n",
      "    (tanh): Tanh()\n",
      "    (dropout_hidden1): MyDropout(\n",
      "    )\n",
      "    (hidden1): Linear(in_features=800, out_features=800)\n",
      "    (tanh_hidden1): Tanh()\n",
      "    (final): Linear(in_features=800, out_features=10)\n",
      "  )\n",
      ")\n",
      "Hidden layers: [800, 800]; dropout rates: [0, 0.5]\n",
      "Epoch 1 loss: 0.48942768611013887\n",
      "Test error: 983; test accuracy: 0.9017\n",
      "Epoch 2 loss: 0.3155509954194228\n",
      "Test error: 747; test accuracy: 0.9252\n",
      "Epoch 3 loss: 0.25472818419337273\n",
      "Test error: 600; test accuracy: 0.94\n",
      "Epoch 4 loss: 0.21034733656793833\n",
      "Test error: 532; test accuracy: 0.9468\n",
      "Epoch 5 loss: 0.17931051310772697\n",
      "Test error: 446; test accuracy: 0.9553\n",
      "Epoch 6 loss: 0.15737843487411737\n",
      "Test error: 384; test accuracy: 0.9615\n",
      "Epoch 7 loss: 0.13931373472635944\n",
      "Test error: 362; test accuracy: 0.9638\n",
      "Epoch 8 loss: 0.12569412926211954\n",
      "Test error: 345; test accuracy: 0.9654\n",
      "Epoch 9 loss: 0.11213370617789527\n",
      "Test error: 303; test accuracy: 0.9696\n",
      "Epoch 10 loss: 0.10246992527662466\n",
      "Test error: 271; test accuracy: 0.9729\n",
      "Epoch 11 loss: 0.09547008505711953\n",
      "Test error: 270; test accuracy: 0.973\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5ad74b437114>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, trainset, testset, verbose)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(mlp1[1].model)\n",
    "mlp1[1].fit(trainset, testset);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(mlp1[2].model)\n",
    "mlp1[2].fit(trainset, testset);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IO('results/mlp1_mnist_dropout_results_pytorch.pkl').to_pickle([(mlp.loss_, mlp.test_accuracy, mlp.test_error)\\\n",
    "                                                                for mlp in mlp1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = IO('results/mlp1_mnist_dropout_results_pytorch.pkl').read_pickle()\n",
    "labels = ['no dropout', '50% dropout in hidden layers', '50% dropout in hidden layers + 20% in input layer']\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "for i, r in enumerate(results):\n",
    "    plt.plot(range(1, len(r[2])+1), r[2], '.-', label=labels[i], alpha=0.6);\n",
    "plt.ylim([80, 250]);\n",
    "plt.legend(loc=1);\n",
    "plt.xlabel('Epochs');\n",
    "plt.ylabel('Number of errors');\n",
    "plt.title('Test error on MNIST dataset');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
