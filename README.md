# Tutorial: Dropout as Regularization and Bayesian Approximation

This tutorial aims to give readers a complete view of dropout, which includes the implementation of dropout (in [PyTorch](https://pytorch.org/)), how to use dropout and why dropout is useful. Basically, dropout can (1) reduce overfitting (so test results will be better) and (2) provide model uncertainty like Bayesian models we see in the class (Bayesian Approximation).

Please view our tutorial [here](https://xuwd11.github.io/Dropout_Tutorial_in_PyTorch/).


## References

[1] [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/pdf/1207.0580.pdf), G. E. Hinton, et al., 2012  
[2] [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](https://arxiv.org/pdf/1506.02142.pdf), Y. Gal, and Z. Ghahramani, 2016  
[3] [Dropout: A Simple Way to Prevent Neural Networks from
Overfitting](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf), N. Srivastava, et al., 2014